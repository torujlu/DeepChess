{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing ANNs with TensorFlow\n",
    "# Final Project: Chess Playing Environment for DeepChess\n",
    "# Group 12: Renato Garita Figueiredo, Hamza Kebiri, Turan Orujlu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Turan Orujlu\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import chess\n",
    "import chess.svg\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepChess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepChessData(object):\n",
    "    \n",
    "    def __init__(self, win_file, loss_file):\n",
    "        \n",
    "        win_data = np.load(win_file)['arr_0']\n",
    "        loss_data = np.load(loss_file)['arr_0']\n",
    "        \n",
    "        # Small value to avoid zero division\n",
    "        epsilon = 1e-9\n",
    "        \n",
    "        # Take win data samples for population moments calculation\n",
    "        # It is computationally expensive to do it for the full data\n",
    "        np.random.seed(0)\n",
    "        indices_w = np.random.choice(len(win_data), size = 250000, replace = False)\n",
    "        win_samples = win_data[indices_w]\n",
    "        \n",
    "        # Clean the cache\n",
    "        del win_data\n",
    "        del indices_w\n",
    "        \n",
    "        # Take loss data samples for population moments calculation\n",
    "        # It is computationally expensive to do it for the full data\n",
    "        np.random.seed(1)\n",
    "        indices_l = np.random.choice(len(loss_data), size = 250000, replace = False)\n",
    "        loss_samples = loss_data[indices_l]\n",
    "        \n",
    "        # Clean the cache\n",
    "        del loss_data\n",
    "        del indices_l\n",
    "        \n",
    "        # Combine win and loss samples\n",
    "        samples = np.concatenate((win_samples, loss_samples))\n",
    "        \n",
    "        # Clean the cache\n",
    "        del win_samples\n",
    "        del loss_samples\n",
    "        \n",
    "        # Calculate population moments\n",
    "        self.avg = np.mean(samples, 0)\n",
    "        self.std = np.std(samples, 0) + epsilon\n",
    "        \n",
    "        # Clean the cache\n",
    "        del samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class that defines the behavior of an Autoencoder\n",
    "class Autoencoder(object):\n",
    "    \n",
    "    # Encode input with input_size as an output with output_size\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        # Defining the hyperparameters\n",
    "        self.input_size = input_size # The input size\n",
    "        self.output_size = output_size # The output size\n",
    "        \n",
    "        # Training weights and biases\n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "        \n",
    "        # Encoder\n",
    "        with tf.variable_scope('encoder_vars'):\n",
    "            \n",
    "            # ReLU weights\n",
    "            var_init = tf.truncated_normal_initializer(stddev = 2 / input_size)\n",
    "            self.weights['encoder_h'] = tf.get_variable('encoder_h_' + str(input_size) + '_-_' + str(output_size),\n",
    "                                                        [input_size, output_size],\n",
    "                                                        tf.float32,\n",
    "                                                        var_init)\n",
    "            \n",
    "            # ReLU biases\n",
    "            var_init = tf.constant_initializer(0.01)\n",
    "            self.biases['encoder_b'] = tf.get_variable('encoder_b' + str(output_size),\n",
    "                                                       [output_size],\n",
    "                                                       tf.float32,\n",
    "                                                       var_init)\n",
    "        \n",
    "        # Decoder\n",
    "        with tf.variable_scope('decoder_vars'):\n",
    "\n",
    "            # ReLU weights\n",
    "            var_init = tf.truncated_normal_initializer(stddev = 2 / input_size)\n",
    "            self.weights['decoder_h'] = tf.get_variable('decoder_h' + str(output_size) + '_-_' + str(input_size),\n",
    "                                                        [output_size, input_size],\n",
    "                                                        tf.float32,\n",
    "                                                        var_init)\n",
    "            \n",
    "            # ReLU biases\n",
    "            var_init = tf.constant_initializer(0.01)\n",
    "            self.biases['decoder_b'] = tf.get_variable('decoder_b' + str(input_size),\n",
    "                                                       [input_size], \n",
    "                                                       tf.float32,\n",
    "                                                       var_init)\n",
    "    \n",
    "        # Input\n",
    "        self.X = tf.placeholder(tf.float32, [None, self.input_size])\n",
    "        \n",
    "        # Stage indicator for batch normalization\n",
    "        self.is_training = tf.placeholder(tf.bool, None)\n",
    "\n",
    "        # Construct model\n",
    "        with tf.variable_scope('encoder_op'):\n",
    "            self.encoder_op = self.encoder(self.X, True)\n",
    "        with tf.variable_scope('decoder_op'):\n",
    "            decoder_op = self.decoder(self.encoder_op, True)\n",
    "\n",
    "        # Prediction\n",
    "        y_pred = decoder_op\n",
    "    \n",
    "    # Batch Normalization with population parameters (non-training)\n",
    "    def _pop_batch_norm(self, x, pop_mean, pop_var, offset, scale):\n",
    "            return tf.nn.batch_normalization(x, pop_mean, pop_var, offset, scale, 1e-6)\n",
    "\n",
    "    # Batch Normalization with batch parameters (training)\n",
    "    def _batch_norm(self, x, pop_mean, pop_var, mean, var, offset, scale):\n",
    "        decay = 0.99\n",
    "\n",
    "        dependency_1 = tf.assign(pop_mean, pop_mean * decay + mean * (1 - decay))\n",
    "        dependency_2 = tf.assign(pop_var, pop_var * decay + var * (1 - decay))\n",
    "\n",
    "        with tf.control_dependencies([dependency_1, dependency_2]):\n",
    "            return tf.nn.batch_normalization(x, mean, var, offset, scale, 1e-6)\n",
    "\n",
    "    # Batch Normalization\n",
    "    def _batch_normalize(self, x, axes):\n",
    "        depth = x.shape[-1]\n",
    "        mean, var = tf.nn.moments(x, axes = axes)\n",
    "\n",
    "        var_init = tf.constant_initializer(0.0)\n",
    "        offset = tf.get_variable('offset', [depth], tf.float32, var_init)\n",
    "        var_init = tf.constant_initializer(1.0)\n",
    "        scale = tf.get_variable('scale', [depth], tf.float32, var_init)\n",
    "\n",
    "        pop_mean = tf.get_variable('pop_mean', [depth], initializer = tf.zeros_initializer(), trainable = False)\n",
    "        pop_var = tf.get_variable('pop_var', [depth], initializer = tf.ones_initializer(), trainable = False)\n",
    "\n",
    "        return tf.cond(\n",
    "            self.is_training,\n",
    "            lambda: self._batch_norm(x, pop_mean, pop_var, mean, var, offset, scale),\n",
    "            lambda: self._pop_batch_norm(x, pop_mean, pop_var, offset, scale)\n",
    "        )\n",
    "    \n",
    "    # Building the encoder: encode the input with ReLU activation\n",
    "    def encoder(self, x, normalize = False):\n",
    "\n",
    "        activation = tf.add(tf.matmul(x, self.weights['encoder_h']), self.biases['encoder_b'])\n",
    "\n",
    "        # Batch Normalization\n",
    "        if normalize:\n",
    "            activation = self._batch_normalize(activation, [0])\n",
    "\n",
    "        return tf.nn.relu(activation)\n",
    "\n",
    "    # Building the decoder: decode the input with ReLU activation\n",
    "    def decoder(self, x, normalize = False):\n",
    "\n",
    "        activation = tf.add(tf.matmul(x, self.weights['decoder_h']), self.biases['decoder_b'])\n",
    "\n",
    "        # Batch Normalization\n",
    "        if normalize:\n",
    "            activation = self._batch_normalize(activation, [0])\n",
    "\n",
    "        return tf.nn.relu(activation)\n",
    "    \n",
    "    # Encoder output method for the Deep Belief Network\n",
    "    def encoder_output(self, sess, X):\n",
    "        \n",
    "        return sess.run(self.encoder_op, feed_dict = {self.X: X, self.is_training: False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Belief Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class that defines the behavior of a Deep Belief Network\n",
    "class DeepBeliefNetwork(object):\n",
    "    \n",
    "    # Create layers of Autoencoders with sizes dbn_layer_sizes\n",
    "    def __init__(self, dbn_layer_sizes):\n",
    "        \n",
    "        print(\"Initializing the Deep Belief Network Data Flow Graph ...\")\n",
    "        \n",
    "        # Create list to hold the Autoencoders\n",
    "        self.autoencoders = []\n",
    "        \n",
    "        # For each Autoencoder we want to generate\n",
    "        for i in range(len(dbn_layer_sizes) - 1):\n",
    "            with tf.variable_scope('Autoencoder_' + str(i + 1)):\n",
    "                input_size = dbn_layer_sizes[i]\n",
    "                output_size = dbn_layer_sizes[i + 1]\n",
    "                print(\"Autoencoder \", i + 1, \": \", input_size, \"->\", output_size, \"->\", input_size)\n",
    "                self.autoencoders.append(Autoencoder(input_size, output_size))\n",
    "                \n",
    "        print(\"The Deep Belief Network Data Flow Graph is initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class that defines the behavior of a Siamese Network\n",
    "class SiameseNetwork(object):\n",
    "    \n",
    "    # Stack fin_layer_sizes on top of a pair of deep_belief_network\n",
    "    def __init__(self, deep_belief_network, fin_layer_sizes):\n",
    "        \n",
    "        print(\"Initializing the Siamese Network Data Flow Graph ...\")\n",
    "        \n",
    "        # List to hold stage indicators for batch normalization in the Deep Belief Network\n",
    "        self.is_training_list = []\n",
    "        \n",
    "        # Deep Belief Network\n",
    "        for i, autoencoder in enumerate(deep_belief_network.autoencoders):\n",
    "            print(\"DBN Layer \", i + 1, \": \", autoencoder.input_size, \"->\", autoencoder.output_size)\n",
    "            self.is_training_list.append(autoencoder.is_training)\n",
    "            \n",
    "        # Training weights and biases\n",
    "        self.fin_weights = []\n",
    "        self.fin_biases = []\n",
    "        \n",
    "        # Final layers\n",
    "        with tf.variable_scope('final_layers'):\n",
    "\n",
    "            input_size = 2 * autoencoder.output_size\n",
    "            for i, fin_layer_size in enumerate(fin_layer_sizes):\n",
    "                output_size = fin_layer_size\n",
    "                \n",
    "                # ReLU weights\n",
    "                var_init = tf.truncated_normal_initializer(stddev = 2 / input_size)\n",
    "                \n",
    "                # Non-ReLU weights for the last layer\n",
    "                if (i == len(fin_layer_sizes) - 1):\n",
    "                    var_init = tf.truncated_normal_initializer(stddev = input_size ** (-1/2))                    \n",
    "                \n",
    "                self.fin_weights.append(tf.get_variable('fin_weights_' + str(i) + '_' + str(input_size) + '_-_' + str(output_size),\n",
    "                                                        [input_size, output_size],\n",
    "                                                        tf.float32,\n",
    "                                                        var_init))\n",
    "                \n",
    "                # ReLU biases\n",
    "                var_init = tf.constant_initializer(0.01)\n",
    "                \n",
    "                # Non-ReLU biases for the last layer\n",
    "                if (i == len(fin_layer_sizes) - 1):\n",
    "                    var_init = tf.constant_initializer(0.0)\n",
    "                \n",
    "                self.fin_biases.append(tf.get_variable('fin_biases_' + str(i) + '_' + str(output_size),\n",
    "                                                       [output_size],\n",
    "                                                       tf.float32,\n",
    "                                                       var_init))\n",
    "                \n",
    "                print(\"Final Layer \", i + 1, \": \", input_size, \"->\", output_size)\n",
    "                input_size = output_size\n",
    "            \n",
    "        # First and second inputs\n",
    "        self.X1 = tf.placeholder(tf.float32, [None, deep_belief_network.autoencoders[0].input_size])\n",
    "        self.X2 = tf.placeholder(tf.float32, [None, deep_belief_network.autoencoders[0].input_size])\n",
    "        \n",
    "        # Stage indicator for batch normalization in final layers\n",
    "        self.is_training = tf.placeholder(tf.bool, None)\n",
    "\n",
    "        # Construct model\n",
    "        dbn_forward_pass_op1 = self.dbn_forward_pass(self.X1, deep_belief_network.autoencoders)\n",
    "        dbn_forward_pass_op2 = self.dbn_forward_pass(self.X2, deep_belief_network.autoencoders)\n",
    "        X = tf.concat([dbn_forward_pass_op1, dbn_forward_pass_op2], axis = 1)\n",
    "        input_X = X\n",
    "        for i in range(len(fin_layer_sizes) - 1):\n",
    "            with tf.variable_scope('fin_forward_pass_op_' + str(i + 1)):\n",
    "                fin_weights = self.fin_weights[i]\n",
    "                fin_biases = self.fin_biases[i]\n",
    "                input_X = self.fin_forward_pass(input_X, fin_weights, fin_biases, True, tf.nn.relu)\n",
    "        fin_weights = self.fin_weights[-1]\n",
    "        fin_biases = self.fin_biases[-1]\n",
    "        with tf.variable_scope('fin_forward_pass_op_' + str(len(fin_layer_sizes))):\n",
    "            self.fin_forward_pass_op = self.fin_forward_pass(input_X, fin_weights, fin_biases, True)\n",
    "        \n",
    "        print(\"The Siamese Network Data Flow Graph is initialized!\")\n",
    "        \n",
    "    # Batch Normalization with population parameters (non-training)\n",
    "    def _pop_batch_norm(self, x, pop_mean, pop_var, offset, scale):\n",
    "            return tf.nn.batch_normalization(x, pop_mean, pop_var, offset, scale, 1e-6)\n",
    "\n",
    "    # Batch Normalization with batch parameters (training)\n",
    "    def _batch_norm(self, x, pop_mean, pop_var, mean, var, offset, scale):\n",
    "        decay = 0.99\n",
    "\n",
    "        dependency_1 = tf.assign(pop_mean, pop_mean * decay + mean * (1 - decay))\n",
    "        dependency_2 = tf.assign(pop_var, pop_var * decay + var * (1 - decay))\n",
    "\n",
    "        with tf.control_dependencies([dependency_1, dependency_2]):\n",
    "            return tf.nn.batch_normalization(x, mean, var, offset, scale, 1e-6)\n",
    "\n",
    "    # Batch Normalization\n",
    "    def _batch_normalize(self, x, axes):\n",
    "        depth = x.shape[-1]\n",
    "        mean, var = tf.nn.moments(x, axes = axes)\n",
    "\n",
    "        var_init = tf.constant_initializer(0.0)\n",
    "        offset = tf.get_variable('offset', [depth], tf.float32, var_init)\n",
    "        var_init = tf.constant_initializer(1.0)\n",
    "        scale = tf.get_variable('scale', [depth], tf.float32, var_init)\n",
    "\n",
    "        pop_mean = tf.get_variable('pop_mean', [depth], initializer = tf.zeros_initializer(), trainable = False)\n",
    "        pop_var = tf.get_variable('pop_var', [depth], initializer = tf.ones_initializer(), trainable = False)\n",
    "\n",
    "        return tf.cond(\n",
    "            self.is_training,\n",
    "            lambda: self._batch_norm(x, pop_mean, pop_var, mean, var, offset, scale),\n",
    "            lambda: self._pop_batch_norm(x, pop_mean, pop_var, offset, scale)\n",
    "        )    \n",
    "    \n",
    "    # Pass the input through the Deep Belief Network\n",
    "    def dbn_forward_pass(self, x, autoencoders):\n",
    "\n",
    "        output = x\n",
    "\n",
    "        for i, autoencoder in enumerate(autoencoders):\n",
    "            with tf.variable_scope('Autoencoder_' + str(i + 1)):\n",
    "                with tf.variable_scope('encoder_op', reuse = True):\n",
    "                    output = autoencoder.encoder(output, True)\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Building final layers: pass the input through the layers with given activation functions\n",
    "    def fin_forward_pass(self, x, weights, biases, normalize = False, activation_function = None):\n",
    "        activation = tf.add(tf.matmul(x, weights), biases)\n",
    "\n",
    "        # Batch Normalization\n",
    "        if normalize:\n",
    "            activation = self._batch_normalize(activation, [0])\n",
    "\n",
    "        return activation_function(activation) if callable(activation_function) else activation\n",
    "    \n",
    "    # Evaluate the Siamese Network\n",
    "    def evaluate(self, sess, X1, X2):\n",
    "        \n",
    "        feed_dict = {self.X1: X1, self.X2: X2, self.is_training: False}\n",
    "        for is_training in self.is_training_list:\n",
    "            feed_dict[is_training] = False\n",
    "        \n",
    "        return sess.run(self.fin_forward_pass_op, feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class that defines the behavior of the Minimax Algorithm\n",
    "# with comparison-based alpha-beta search\n",
    "class Minimax(object):\n",
    "    \n",
    "    def __init__(self, sess, deep_chess, deep_chess_data):\n",
    "        \n",
    "        self.sess = sess # tensorflow session\n",
    "        self.deep_chess = deep_chess # heuristic function\n",
    "        self.avg = deep_chess_data.avg # population average for input normalization\n",
    "        self.std = deep_chess_data.std # population standard deviation for input normalization\n",
    "    \n",
    "    # Converts the chess.Board representation of the position\n",
    "    # to a binary bit-string representation called bitboard.\n",
    "    def _normalized_bitboard(self, board):\n",
    "        \n",
    "        # binary bit-string\n",
    "        bitboard = np.zeros(773, dtype=bool)\n",
    "\n",
    "        # every piece on the board gets its bit-representation\n",
    "        piece_map = board.piece_map()\n",
    "        for square in piece_map.keys():\n",
    "            piece = piece_map[square]\n",
    "            color = int(piece.color)\n",
    "            piece_type = piece.piece_type\n",
    "            index = (1 - color) * 6 * 64 + (piece_type - 1) * 64 + square\n",
    "            bitboard[index] = 1\n",
    "\n",
    "        # side to move\n",
    "        bitboard[768] = int(board.turn)\n",
    "\n",
    "        # castling rights\n",
    "        bitboard[769] = int(board.has_kingside_castling_rights(chess.WHITE))\n",
    "        bitboard[770] = int(board.has_queenside_castling_rights(chess.WHITE))\n",
    "        bitboard[771] = int(board.has_kingside_castling_rights(chess.BLACK))\n",
    "        bitboard[772] = int(board.has_queenside_castling_rights(chess.BLACK))\n",
    "\n",
    "        # normalize the bitboard representation\n",
    "        bitboard = (bitboard - self.avg) / self.std\n",
    "        \n",
    "        return np.reshape(bitboard, (1, 773))\n",
    "    \n",
    "    # Fail-hard alpha-beta algorithm (max player)\n",
    "    def alpha_beta_max(self, board, alpha_pos, alpha_move, beta_pos, beta_move, depth):\n",
    "        \n",
    "        # Return the normalized bitboard representation when\n",
    "        # the node is a final one or the depth limit is reached\n",
    "        if depth == 0 or board.is_game_over():\n",
    "            \n",
    "            return self._normalized_bitboard(board), None\n",
    "\n",
    "        # Copy the board\n",
    "        child_board = board.copy()\n",
    "        \n",
    "        # Randomize legal moves\n",
    "        legal_moves = random.sample(list(board.legal_moves), len(list(board.legal_moves)))\n",
    "\n",
    "        # For every legal move,\n",
    "        for move in legal_moves:\n",
    "            \n",
    "            # Make the move on the copy\n",
    "            child_board.push(move)\n",
    "\n",
    "            # Get the result of the move of the min player\n",
    "            pos, _ = self.alpha_beta_min(child_board, alpha_pos, alpha_move, beta_pos, beta_move, depth - 1)\n",
    "            \n",
    "            # If the resulting position is better than the beta position, return the\n",
    "            # beta position which is the least best position for the max player that\n",
    "            # the min player is assured of. This represents fail-hard beta-cutoff.\n",
    "            # Having a guaranteed worse outcome for the max player, the min player\n",
    "            # won't enter this node since the max player has at least one move in\n",
    "            # this node with an outcome that dominates beta position. Note: None\n",
    "            # corresponds to an infinitely good outcome for the max player when\n",
    "            # assigned to the beta position.\n",
    "            if beta_pos is not None:\n",
    "                comparison = np.squeeze(self.deep_chess.evaluate(self.sess, pos, beta_pos))\n",
    "                if comparison[0] >= comparison[1]:\n",
    "                    return beta_pos, beta_move\n",
    "\n",
    "            # If the resulting position is better than the alpha position, assign it to\n",
    "            # to the alpha position which is the least worst position for the max player\n",
    "            # that the max player is assured of. After entering this node, the max player\n",
    "            # can make at least this move that results in a better position than the alpha\n",
    "            # position. Hence, the resulting position is the new guaranteed least worst case.\n",
    "            # Note: None corresponds to an infinitely bad outcome for the max player when\n",
    "            # assigned to the alpha position.\n",
    "            if alpha_pos is None:\n",
    "                \n",
    "                alpha_pos = pos\n",
    "                alpha_move = move\n",
    "            else:\n",
    "                comparison = np.squeeze(self.deep_chess.evaluate(self.sess, pos, alpha_pos))\n",
    "                if comparison[0] > comparison[1]:\n",
    "                    alpha_pos = pos\n",
    "                    alpha_move = move\n",
    "\n",
    "            # Unmake the move\n",
    "            child_board = board.copy()\n",
    "\n",
    "        # Return the alpha position (and the corresponding move) as\n",
    "        # it is the guaranteed worst-case scenario for the max player.\n",
    "        return alpha_pos, alpha_move\n",
    "        \n",
    "    # Fail-hard alpha-beta algorithm (min player)\n",
    "    def alpha_beta_min(self, board, alpha_pos, alpha_move, beta_pos, beta_move, depth):\n",
    "            \n",
    "        # Return the normalized bitboard representation when\n",
    "        # the node is a final one or the depth limit is reached\n",
    "        if depth == 0 or board.is_game_over():\n",
    "            \n",
    "            return self._normalized_bitboard(board), None\n",
    "\n",
    "         # Copy the board\n",
    "        child_board = board.copy()\n",
    "        \n",
    "        # Randomize legal moves\n",
    "        legal_moves = random.sample(list(board.legal_moves), len(list(board.legal_moves)))\n",
    "\n",
    "        # For every legal move,\n",
    "        for move in legal_moves:\n",
    "            \n",
    "            # Make the move on the copy\n",
    "            child_board.push(move)\n",
    "            \n",
    "            # Get the result of the move of the max player\n",
    "            pos, _ = self.alpha_beta_max(child_board, alpha_pos, alpha_move, beta_pos, beta_move, depth - 1)\n",
    "            \n",
    "            # If the resulting position is worse than the alpha position, return the\n",
    "            # alpha position which is the least worst position for the max player that\n",
    "            # the max player is assured of. This represents fail-hard alpha-cutoff.\n",
    "            # Having a guaranteed better outcome for the itself, the max player\n",
    "            # won't enter this node since the min player has at least one move in\n",
    "            # this node with an outcome that is worse than the alpha position.\n",
    "            # Note: None corresponds to an infinitely bad outcome for the max player\n",
    "            # when assigned to the alpha position.\n",
    "            if alpha_pos is not None:\n",
    "                comparison = np.squeeze(self.deep_chess.evaluate(self.sess, pos, alpha_pos))\n",
    "                if comparison[0] <= comparison[1]:\n",
    "                    return alpha_pos, alpha_move\n",
    "\n",
    "            # If the resulting position is worse than the beta position, assign it to\n",
    "            # to the beta position which is the least best position for the max player\n",
    "            # that the min player is assured of. After entering this node, the min player\n",
    "            # can make at least this move that results in a worse position for the max player\n",
    "            # than the beta position. Hence, the resulting position is the new guaranteed\n",
    "            # least worst case. Note: None corresponds to an infinitely good outcome for\n",
    "            # the max player when assigned to the beta position.\n",
    "            if beta_pos is None:\n",
    "                beta_pos = pos\n",
    "                beta_move = move\n",
    "            else:\n",
    "                comparison = np.squeeze(self.deep_chess.evaluate(self.sess, pos, beta_pos))\n",
    "                if comparison[0] < comparison[1]:\n",
    "                    beta_pos = pos\n",
    "                    beta_move = move\n",
    "\n",
    "            # Unmake the move\n",
    "            child_board = board.copy()\n",
    "\n",
    "        # Return the beta position (and the corresponding move) as\n",
    "        # it is the guaranteed worst-case scenario for the min player.\n",
    "        return beta_pos, beta_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chess Playing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class that defines the behavior\n",
    "# of the Chess Playing Environment\n",
    "class ChessPlayingEnvironment(object):\n",
    "    \n",
    "    def __init__(self, minimax, depth):\n",
    "    \n",
    "        self.board = chess.Board() # initialize a new playing board\n",
    "        self.minimax = minimax # Minimax Algorithm\n",
    "        self.depth = depth # depth limit for the Minimax Algorithm\n",
    "\n",
    "    # method for game simulation\n",
    "    def play(self):\n",
    "        \n",
    "        # new game flag\n",
    "        new_game = True\n",
    "        \n",
    "        # check if there is an abandoned game\n",
    "        while True:\n",
    "            try:\n",
    "                # when an abandoned game is detected,\n",
    "                # ask if the player wants to continue\n",
    "                move = self.board.pop()\n",
    "                self.board.push(move)\n",
    "                if not self.board.is_game_over():\n",
    "                    answer = input(\"Do you want to continue the previously abandoned game? \\n\")\n",
    "                    if answer in ['Yes', 'yes']:\n",
    "                        # if yes, change the new game flag\n",
    "                        new_game = False\n",
    "                        # clear the output of the cell\n",
    "                        clear_output(wait = True)\n",
    "                        # display the playing board\n",
    "                        display(self.board)\n",
    "                        break\n",
    "                    elif answer in ['No', 'no']:\n",
    "                        # if not, initialize a new playing board\n",
    "                        self.board = chess.Board()\n",
    "                        # clear the output of the cell\n",
    "                        clear_output(wait = True)\n",
    "                        break\n",
    "                    else:\n",
    "                        raise ValueError\n",
    "                else:\n",
    "                    clear_output(wait = True)\n",
    "                    self.board = chess.Board()\n",
    "            except IndexError:\n",
    "                break\n",
    "            except ValueError:\n",
    "                # clear the output of the cell\n",
    "                clear_output(wait = True)\n",
    "                print(\"Please enter a valid answer!\")\n",
    "        \n",
    "        # get player's piece preferences\n",
    "        while new_game:\n",
    "            try:\n",
    "                player = input(\"Who plays with white pieces? DeepChess or you? \\n\")\n",
    "                if player in ['me', 'Me', 'i', 'I']:\n",
    "                    # DeepChess is not playing with white pieces\n",
    "                    self.white = False\n",
    "                    # it is the player's turn to make a move\n",
    "                    self.my_turn = True\n",
    "                elif player in ['Deep Chess', 'deep chess', 'DeepChess', 'Deep chess', 'deepchess', 'Deepchess']:\n",
    "                    # DeepChess is playing with white pieces\n",
    "                    self.white = True\n",
    "                    # it is not the player's turn to make a move\n",
    "                    self.my_turn = False\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            except ValueError:\n",
    "                # clear the output of the cell\n",
    "                clear_output(wait = True)\n",
    "                print(\"Please enter a valid answer!\")\n",
    "                continue\n",
    "            break\n",
    "        \n",
    "        # display the playing board\n",
    "        clear_output(wait = True)\n",
    "        # clear the output of the cell\n",
    "        display(self.board)\n",
    "\n",
    "        # abandoned game flag\n",
    "        abandoned = False \n",
    "        \n",
    "        # main game loop: as long as the game is not over or abandoned,\n",
    "        while (not abandoned) and (not self.board.is_game_over()):\n",
    "            # if it is not the player's turn\n",
    "            if not self.my_turn:\n",
    "                print(\"DeepChess is thinking ...\")\n",
    "                if self.white:\n",
    "                    # run alpha_beta_max to get DeepChess' move when its playing with whites\n",
    "                    _, move = self.minimax.alpha_beta_max(self.board, None, None, None, None, self.depth)\n",
    "                else:\n",
    "                    # run alpha_beta_min to get DeepChess' move when its playing with blacks\n",
    "                    _, move = self.minimax.alpha_beta_min(self.board, None, None, None, None, self.depth)\n",
    "            # if it is the player's turn\n",
    "            else:\n",
    "                # get the player's move\n",
    "                while True:\n",
    "                    try:\n",
    "                        move = input(\"Your move: \")\n",
    "                        if move in ['Quit', 'quit']:\n",
    "                            # set the abandoned game flag to\n",
    "                            # True when the player quits\n",
    "                            abandoned = True\n",
    "                            # clear the output of the cell\n",
    "                            clear_output(wait = True)\n",
    "                            # display the playing board\n",
    "                            display(self.board)\n",
    "                            print(\"The game was abandoned.\")\n",
    "                        else:\n",
    "                            # check if the move is a legal move\n",
    "                            move = chess.Move.from_uci(move)\n",
    "                            if move not in self.board.legal_moves:\n",
    "                                raise ValueError\n",
    "                    except ValueError:\n",
    "                        # clear the output of the cell\n",
    "                        clear_output(wait = True)\n",
    "                        # display the playing board\n",
    "                        display(self.board)\n",
    "                        print(\"Please enter a legal move!\")\n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "            # as long as the game is not over or abandoned,\n",
    "            if (not abandoned) and (not self.board.is_game_over()):\n",
    "                # change the turn\n",
    "                self.my_turn = not self.my_turn\n",
    "                # update the board\n",
    "                self.board.push(move)\n",
    "                # clear the output of the cell\n",
    "                clear_output(wait = True)\n",
    "                # display the playing board\n",
    "                display(self.board)\n",
    "        # end game conditions\n",
    "        if self.board.is_checkmate():\n",
    "            if not self.my_turn:\n",
    "                print(\"You won!\")\n",
    "            else:\n",
    "                print(\"You lost!\")\n",
    "        elif self.board.is_stalemate() or self.board.is_insufficient_material():\n",
    "            print(\"It's a draw!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class instances and TensorFlow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Deep Belief Network Data Flow Graph ...\n",
      "Autoencoder  1 :  773 -> 100 -> 773\n",
      "Autoencoder  2 :  100 -> 100 -> 100\n",
      "Autoencoder  3 :  100 -> 100 -> 100\n",
      "The Deep Belief Network Data Flow Graph is initialized!\n",
      "Initializing the Siamese Network Data Flow Graph ...\n",
      "DBN Layer  1 :  773 -> 100\n",
      "DBN Layer  2 :  100 -> 100\n",
      "DBN Layer  3 :  100 -> 100\n",
      "Final Layer  1 :  200 -> 100\n",
      "Final Layer  2 :  100 -> 100\n",
      "Final Layer  3 :  100 -> 2\n",
      "The Siamese Network Data Flow Graph is initialized!\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Import Deep Chess Data\n",
    "deep_chess_data = DeepChessData('./data/win_data.npz', './data/loss_data.npz')\n",
    "\n",
    "# Create Pos2Vec and DeepChess instances\n",
    "pos_2_vec = DeepBeliefNetwork(dbn_layer_sizes = [773, 100, 100, 100])\n",
    "deep_chess = SiameseNetwork(deep_belief_network = pos_2_vec, fin_layer_sizes = [100, 100, 2])\n",
    "\n",
    "# Instantiate a tf session and a model saver\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver(keep_checkpoint_every_n_hours = 2)\n",
    "\n",
    "# Initialize the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Restore the model\n",
    "saver.restore(sess, './model/model.ckpt')\n",
    "\n",
    "# Create a Minimax instance\n",
    "minimax = Minimax(sess, deep_chess, deep_chess_data)\n",
    "\n",
    "# Create a ChessPlayingEnvironment instance\n",
    "chess_playing_environment = ChessPlayingEnvironment(minimax, depth = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play\n",
    "#### Enter your move as the concatenation of the coordinate of the piece and the coordinate of the square that you would like to move the piece to. For example, to move the white pawn on 'e2' to 'e4', write 'e2e4'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"400\" version=\"1.1\" viewBox=\"0 0 400 400\" width=\"400\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><g class=\"white pawn\" id=\"white-pawn\"><path d=\"M22 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38-1.95 1.12-3.28 3.21-3.28 5.62 0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-linecap=\"round\" stroke-width=\"1.5\" /></g><g class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" id=\"white-knight\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" style=\"fill:#000000; stroke:#000000;\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" /></g><g class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" id=\"white-bishop\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" id=\"white-rook\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" id=\"white-queen\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" id=\"white-king\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g class=\"black pawn\" id=\"black-pawn\"><path d=\"M22 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38-1.95 1.12-3.28 3.21-3.28 5.62 0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" stroke=\"#000\" stroke-linecap=\"round\" stroke-width=\"1.5\" /></g><g class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" id=\"black-knight\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" style=\"fill:#ececec; stroke:#ececec;\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" id=\"black-bishop\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" id=\"black-rook\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-linejoin=\"miter\" stroke-width=\"1\" /></g><g class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" id=\"black-queen\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" id=\"black-king\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect class=\"square dark a1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"335\" /><use transform=\"translate(20, 335)\" xlink:href=\"#white-rook\" /><rect class=\"square light b1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"335\" /><rect class=\"square dark c1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"335\" /><use transform=\"translate(110, 335)\" xlink:href=\"#white-bishop\" /><rect class=\"square light d1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"335\" /><use transform=\"translate(155, 335)\" xlink:href=\"#white-queen\" /><rect class=\"square dark e1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"335\" /><use transform=\"translate(200, 335)\" xlink:href=\"#white-king\" /><rect class=\"square light f1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"335\" /><use transform=\"translate(245, 335)\" xlink:href=\"#white-bishop\" /><rect class=\"square dark g1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"335\" /><rect class=\"square light h1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"335\" /><use transform=\"translate(335, 335)\" xlink:href=\"#white-rook\" /><rect class=\"square light a2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"290\" /><use transform=\"translate(20, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark b2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"290\" /><use transform=\"translate(65, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square light c2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"290\" /><use transform=\"translate(110, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark lastmove d2\" fill=\"#aaa23b\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"290\" /><rect class=\"square light e2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"290\" /><rect class=\"square dark f2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"290\" /><use transform=\"translate(245, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square light g2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"290\" /><use transform=\"translate(290, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark h2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"290\" /><use transform=\"translate(335, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark a3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"245\" /><rect class=\"square light b3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"245\" /><rect class=\"square dark c3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"245\" /><use transform=\"translate(110, 245)\" xlink:href=\"#white-knight\" /><rect class=\"square light d3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"245\" /><rect class=\"square dark e3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"245\" /><rect class=\"square light f3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"245\" /><use transform=\"translate(245, 245)\" xlink:href=\"#white-knight\" /><rect class=\"square dark g3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"245\" /><rect class=\"square light h3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"245\" /><rect class=\"square light a4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"200\" /><rect class=\"square dark b4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"200\" /><rect class=\"square light c4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"200\" /><rect class=\"square dark lastmove d4\" fill=\"#aaa23b\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"200\" /><use transform=\"translate(155, 200)\" xlink:href=\"#white-pawn\" /><rect class=\"square light e4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"200\" /><use transform=\"translate(200, 200)\" xlink:href=\"#black-knight\" /><rect class=\"square dark f4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"200\" /><rect class=\"square light g4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"200\" /><rect class=\"square dark h4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"200\" /><rect class=\"square dark a5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"155\" /><rect class=\"square light b5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"155\" /><rect class=\"square dark c5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"155\" /><rect class=\"square light d5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"155\" /><rect class=\"square dark e5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"155\" /><use transform=\"translate(200, 155)\" xlink:href=\"#black-pawn\" /><rect class=\"square light f5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"155\" /><rect class=\"square dark g5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"155\" /><rect class=\"square light h5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"155\" /><rect class=\"square light a6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"110\" /><rect class=\"square dark b6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"110\" /><rect class=\"square light c6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"110\" /><rect class=\"square dark d6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"110\" /><rect class=\"square light e6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"110\" /><rect class=\"square dark f6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"110\" /><rect class=\"square light g6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"110\" /><rect class=\"square dark h6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"110\" /><rect class=\"square dark a7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"65\" /><use transform=\"translate(20, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square light b7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"65\" /><use transform=\"translate(65, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square dark c7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"65\" /><use transform=\"translate(110, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square light d7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"65\" /><use transform=\"translate(155, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square dark e7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"65\" /><rect class=\"square light f7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"65\" /><use transform=\"translate(245, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square dark g7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"65\" /><use transform=\"translate(290, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square light h7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"65\" /><use transform=\"translate(335, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square light a8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"20\" /><use transform=\"translate(20, 20)\" xlink:href=\"#black-rook\" /><rect class=\"square dark b8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"20\" /><use transform=\"translate(65, 20)\" xlink:href=\"#black-knight\" /><rect class=\"square light c8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"20\" /><use transform=\"translate(110, 20)\" xlink:href=\"#black-bishop\" /><rect class=\"square dark d8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"20\" /><use transform=\"translate(155, 20)\" xlink:href=\"#black-queen\" /><rect class=\"square light e8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"20\" /><use transform=\"translate(200, 20)\" xlink:href=\"#black-king\" /><rect class=\"square dark f8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"20\" /><use transform=\"translate(245, 20)\" xlink:href=\"#black-bishop\" /><rect class=\"square light g8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"20\" /><rect class=\"square dark h8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"20\" /><use transform=\"translate(335, 20)\" xlink:href=\"#black-rook\" /><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"42\" y=\"10\">a</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"42\" y=\"390\">a</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"87\" y=\"10\">b</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"87\" y=\"390\">b</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"132\" y=\"10\">c</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"132\" y=\"390\">c</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"177\" y=\"10\">d</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"177\" y=\"390\">d</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"222\" y=\"10\">e</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"222\" y=\"390\">e</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"267\" y=\"10\">f</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"267\" y=\"390\">f</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"312\" y=\"10\">g</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"312\" y=\"390\">g</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"357\" y=\"10\">h</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"357\" y=\"390\">h</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"357\">1</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"357\">1</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"312\">2</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"312\">2</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"267\">3</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"267\">3</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"222\">4</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"222\">4</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"177\">5</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"177\">5</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"132\">6</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"132\">6</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"87\">7</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"87\">7</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"42\">8</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"42\">8</text></svg>"
      ],
      "text/plain": [
       "Board('rnbqkb1r/pppp1ppp/8/4p3/3Pn3/2N2N2/PPP2PPP/R1BQKB1R b KQkq - 0 4')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The game was abandoned.\n"
     ]
    }
   ],
   "source": [
    "# Play against DeepChess\n",
    "chess_playing_environment.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
